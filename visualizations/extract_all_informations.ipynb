{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17fdeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import base64\n",
    "import stable_baselines3\n",
    "from torch import nn\n",
    "from torch.functional import F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.dqn.policies import CnnPolicy\n",
    "\n",
    "import gym\n",
    "\n",
    "from gym.wrappers import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from helper_functions import wrap_env, extract_information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a85b89e",
   "metadata": {},
   "source": [
    "## Extract information from pretrained model on random Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35ccdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = './log/'\n",
    "Path(log_dir).mkdir(exist_ok=True, parents=True)\n",
    "nn_layers = [64,64] \n",
    "learning_rate = 0.001 \n",
    "\n",
    "# Create environment\n",
    "env = gym.make('LunarLander-v2')\n",
    "#You can also load other environments like cartpole, MountainCar, Acrobot. Refer to https://gym.openai.com/docs/ for descriptions.\n",
    "#For example, if you would like to load Cartpole, just replace the above statement with \"env = gym.make('CartPole-v1')\".\n",
    "\n",
    "policy_kwargs = dict(activation_fn=torch.nn.ReLU,\n",
    "                  net_arch=nn_layers)\n",
    "model = DQN(\"MlpPolicy\", #CnnPolicy,\n",
    "            \n",
    "            env,policy_kwargs = policy_kwargs,\n",
    "          learning_rate=learning_rate,\n",
    "          batch_size=1,  #for simplicity, we are not doing batch update.\n",
    "          buffer_size=1, #size of experience of replay buffer. Set to 1 as batch update is not done\n",
    "          learning_starts=1, #learning starts immediately!\n",
    "          gamma=0.99, #discount facto. range is between 0 and 1.\n",
    "          tau = 1,  #the soft update coefficient for updating the target network\n",
    "          target_update_interval=1, #update the target network immediately.\n",
    "          train_freq=(1,\"step\"), #train the network at every step.\n",
    "          max_grad_norm = 10, #the maximum value for the gradient clipping\n",
    "          exploration_initial_eps = 1, #initial value of random action probability\n",
    "          exploration_fraction = 0.5, #fraction of entire training period over which the exploration rate is reduced\n",
    "          gradient_steps = 1, #number of gradient steps\n",
    "          seed = 1, #seed for the pseudo random generators\n",
    "          verbose=1) #Set verbose to 1 to observe training logs. We encourage you to set the verbose to 1.\n",
    "model = model.load('../models/laura_best_panda.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a2b8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('PandaLander-v2', observe_state=True,\n",
    "                                    random_initial_x=True)\n",
    "\n",
    "\n",
    "extract_information(model, env, './original/', 100, save_video = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0febadd2",
   "metadata": {},
   "source": [
    "# Transfer learning on random init of previous model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4529da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-10\n",
    "\n",
    "env = gym.make('PandaLander-v2',\n",
    "               observe_state=True,\n",
    "               random_initial_x=True)\n",
    "callback = EvalCallback(env,log_path = log_dir, deterministic=True) #For evaluating the performance of the agent periodically and logging the results.\n",
    "policy_kwargs = dict(activation_fn=torch.nn.ReLU,\n",
    "                      net_arch=[512,512])\n",
    "model = DQN(\"MlpPolicy\", \n",
    "            \n",
    "            env,policy_kwargs = policy_kwargs,\n",
    "          learning_rate=learning_rate,\n",
    "          batch_size=32,  #for simplicity, we are not doing batch update.\n",
    "          buffer_size=1, #size of experience of replay buffer. Set to 1 as batch update is not done\n",
    "          learning_starts=1, #learning starts immediately!\n",
    "          gamma=0.99, #discount facto. range is between 0 and 1.\n",
    "          tau = 1,  #the soft update coefficient for updating the target network\n",
    "          target_update_interval=1, #update the target network immediately.\n",
    "          train_freq=(1,\"step\"), #train the network at every step.\n",
    "          max_grad_norm = 10, #the maximum value for the gradient clipping\n",
    "          exploration_initial_eps = 1, #initial value of random action probability\n",
    "          exploration_fraction = 0.5, #fraction of entire training period over which the exploration rate is reduced\n",
    "          gradient_steps = 1, #number of gradient steps\n",
    "          seed = 1, #seed for the pseudo random generators\n",
    "          verbose=1) #Set verbose to 1 to observe training logs. We encourage you to set the verbose to 1.\n",
    "\n",
    "model_saved = model.load('../models/laura_best_panda.zip')\n",
    "model.set_parameters(model_saved.get_parameters())\n",
    "\n",
    "model.learning_rate = learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9625636b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=100_000, log_interval=10, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abd305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('PandaLander-v2', observe_state=True,\n",
    "                                    random_initial_x=True)\n",
    "\n",
    "\n",
    "extract_information(model, env, './transfer/', 100, save_video = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b01a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../models/transfered.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2a0b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
